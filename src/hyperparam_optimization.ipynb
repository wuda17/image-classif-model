{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCyxR6GKae03"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def data_loader(data_dir: str, batch_size: int, shuffle: bool=True) \\\n",
        "    -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, List[str]]:\n",
        "    \"\"\"\n",
        "    Takes in a dataset directory and returns a train and test data loader\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir: str\n",
        "        relative path to directory containing dataset\n",
        "    batch_size: int\n",
        "        batch size to load into torch.utils.data.DataLoader\n",
        "    shuffle: bool\n",
        "        whether to shuffle batches of torch.utils.data.DataLoader\n",
        "\n",
        "    Retrns\n",
        "    ------\n",
        "    Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, List[str]]\n",
        "        (train_dataloader, test_dataloader, class_names). class_names is a list of target classes.\n",
        "\n",
        "    \"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4799],\n",
        "        std=[0.2386],\n",
        "    )\n",
        "\n",
        "    # define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    # Load the dataset\n",
        "    test_data = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "    train_data = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    # Get class names\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    # Turn images into data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return train_loader, test_loader, class_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "1tM07AqcNaaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkUogUGgakb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a VGG-16 model.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    \"\"\"\n",
        "    Adapts the VGG-16 architecture for 1 channel grey-scaled images found at the following source:\n",
        "    https://arxiv.org/pdf/1409.1556.pdf?ref=blog.paperspace.com\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512*1*1, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "a_bYcGrFNkND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4gYZhnnanfg"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    acc = results['train_acc']\n",
        "    test_acc = results['test_acc']\n",
        "\n",
        "    number_of_epochs = range(len(results['train_loss']))\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(number_of_epochs, loss, label='train_loss')\n",
        "    plt.plot(number_of_epochs, test_loss, label='test_loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(number_of_epochs, acc, label='train_accuracy')\n",
        "    plt.plot(number_of_epochs, test_acc, label='test_accuracy')\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NTMOXAGyNpzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN960IV5ahl5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains functions for training and testing model.\n",
        "\"\"\"\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Step through one epoch of training step\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        A PyTorch model to be trained.\n",
        "    dataloader: torch.utils.data.DataLoader\n",
        "        A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: torch.nn.Module\n",
        "        A PyTorch loss function to minimize.\n",
        "    optimizer: torch.optim.Optimizer\n",
        "        A PyTorch optimizer to help minimize the loss function.\n",
        "    device: torch.cuda.device\n",
        "        A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        A tuple of training loss and training accuracy metrics. In the form (train_loss, train_accuracy).\n",
        "    \"\"\"\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Calculate and append loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_accuracy += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_accuracy /= len(data_loader)\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.cuda.device) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Step through one epoch of training step\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        A PyTorch model to be tested.\n",
        "    dataloader: torch.utils.data.DataLoader\n",
        "        A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: torch.nn.Module\n",
        "        A PyTorch loss function to minimize.\n",
        "    device: torch.cuda.device\n",
        "        A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        A tuple of test loss and test accuracy metrics. In the form (test_loss, test_accuracy).\n",
        "    \"\"\"\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_test, y_test in data_loader:\n",
        "\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            test_pred_logits = model(X_test)\n",
        "\n",
        "            # Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y_test)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_accuracy += ((test_pred_labels == y_test).sum().item()/len(test_pred_labels))\n",
        "\n",
        "        test_loss /= len(data_loader)\n",
        "        test_accuracy /= len(data_loader)\n",
        "        return test_loss, test_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Entire"
      ],
      "metadata": {
        "id": "wJyxLRzsN-E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          max_lr: float,\n",
        "          device: torch.device,\n",
        "          grad_clip: float =None) -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        A PyTorch model to be trained and tested.\n",
        "    train_loader: torch.utils.data.DataLoader\n",
        "        A DataLoader instance for the model to be trained on.\n",
        "    test_loader: torch.utils.data.DataLoader\n",
        "        A DataLoader instance for the model to be tested on.\n",
        "    optimizer: torch.optim.Optimizer\n",
        "        A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: torch.nn.Module\n",
        "        A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: int\n",
        "        An integer indicating how many epochs to train for.\n",
        "    max_lr: float\n",
        "        A float representing the maximum learning rate for the learing_rate\n",
        "        scheduler.\n",
        "    device: torch.device\n",
        "        A target device to compute on\n",
        "    grad_clip: float = None\n",
        "        A float representing the maximum gradient threshold during training\n",
        "        to stabilize gradient values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, List]\n",
        "        A dictionary of training and testing loss as well as training and\n",
        "        testing accuracy metrics. Each metric has a value in a list for\n",
        "        each epoch.\n",
        "        {train_loss: [...],\n",
        "        train_acc: [...],\n",
        "        test_loss: [...],\n",
        "        test_acc: [...],\n",
        "        lr: [...]}\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_dataloader))\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"lrs\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Learning Rate:\", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           data_loader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer,\n",
        "                                           device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "                                        data_loader=test_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        device=device)\n",
        "\n",
        "        current_lr = get_lr(optimizer)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f} |\"\n",
        "            f\"lr: {current_lr:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"lrs\"].append(current_lr)\n",
        "\n",
        "        # Update learning rate\n",
        "        sched.step()\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "Jf-xkZBPN_vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperopt"
      ],
      "metadata": {
        "id": "Y_IF9FcSOX13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials\n",
        "\n",
        "def optimize_hyperparameters(data_dir, device, num_evals=50):\n",
        "    # Define the search space for hyperparameters\n",
        "    space = {\n",
        "        'num_epochs': hp.quniform('num_epochs', 10, 50, 1),\n",
        "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
        "        'max_lr': hp.loguniform('max_lr', -4, -1),\n",
        "        'grad_clip': hp.uniform('grad_clip', 0.0, 0.5),\n",
        "        'weight_decay': hp.loguniform('weight_decay', -6, -2)\n",
        "    }\n",
        "\n",
        "    # Define your objective function for Hyperopt\n",
        "    def objective(params):\n",
        "        # Create dataloaders from data_setup.py\n",
        "        train_dataloader, test_dataloader, class_names = data_loader(\n",
        "            data_dir=data_dir,\n",
        "            batch_size=params['batch_size']\n",
        "        )\n",
        "\n",
        "        # Create and configure the model using the provided hyperparameters\n",
        "        model = VGG16(num_classes=len(class_names))\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=params['max_lr'], weight_decay=params['weight_decay'],)\n",
        "\n",
        "        # Start training from engine.py\n",
        "        results = train(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            test_dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            epochs=int(params['num_epochs']),\n",
        "            device=device,\n",
        "            max_lr=params['max_lr'],  # Use the sampled learning rate\n",
        "            grad_clip=params['grad_clip']\n",
        "        )\n",
        "\n",
        "        # Return a value to minimize (e.g., negative test accuracy)\n",
        "        return -results['test_acc'][-1]\n",
        "\n",
        "    # Set up Hyperopt for optimization\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=num_evals, trials=trials)\n",
        "\n",
        "    # Retrieve the best hyperparameters\n",
        "    best_num_epochs = int(best['num_epochs'])\n",
        "    best_max_lr = best['max_lr']\n",
        "    best_grad_clip = best['neurons_per_layer']\n",
        "    best_weight_decay = best['weight_decay']\n",
        "\n",
        "    return best_num_epochs, best_max_lr, best_grad_clip, best_weight_decay\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_dir = 'data'\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    best_num_epochs, best_max_lr, best_grad_clip, best_weight_decay = optimize_hyperparameters(data_dir, device)\n",
        "\n",
        "    print(f'Best hyperparameters: '\n",
        "          f'epochs: {best_num_epochs} | '\n",
        "          f'lr: {best_max_lr} | '\n",
        "          f'grad_clip: {best_grad_clip} | '\n",
        "          f'weight_decay: {best_weight_decay}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USE_CPBVOXBo",
        "outputId": "e0d9ee58-4cc3-4d87-9620-415ed319dcf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
            "  0%|          | 0/50 [00:01<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/170498071 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  0%|          | 32768/170498071 [00:00<19:39, 144523.34it/s]\n",
            "\u001b[A\n",
            "  0%|          | 65536/170498071 [00:00<19:23, 146491.55it/s]\n",
            "\u001b[A\n",
            "  0%|          | 98304/170498071 [00:00<19:18, 147087.66it/s]\n",
            "\u001b[A\n",
            "  0%|          | 229376/170498071 [00:00<08:53, 319141.80it/s]\n",
            "\u001b[A\n",
            "  0%|          | 458752/170498071 [00:01<04:59, 567952.47it/s]\n",
            "\u001b[A\n",
            "  0%|          | 851968/170498071 [00:01<02:23, 1181912.69it/s]\n",
            "\u001b[A\n",
            "  1%|          | 1048576/170498071 [00:01<02:19, 1216941.76it/s]\n",
            "\u001b[A\n",
            "  1%|          | 1703936/170498071 [00:01<01:12, 2337996.84it/s]\n",
            "\u001b[A\n",
            "  1%|1         | 2031616/170498071 [00:01<01:08, 2462486.97it/s]\n",
            "\u001b[A\n",
            "  2%|1         | 3080192/170498071 [00:01<00:38, 4399123.12it/s]\n",
            "\u001b[A\n",
            "  2%|2         | 3702784/170498071 [00:01<00:34, 4848236.72it/s]\n",
            "\u001b[A\n",
            "  3%|3         | 5373952/170498071 [00:01<00:20, 8010824.17it/s]\n",
            "\u001b[A\n",
            "  4%|4         | 7110656/170498071 [00:02<00:15, 10567413.41it/s]\n",
            "\u001b[A\n",
            "  5%|4         | 8257536/170498071 [00:02<00:16, 9710679.65it/s] \n",
            "\u001b[A\n",
            "  6%|5         | 10223616/170498071 [00:02<00:15, 10102914.83it/s]\n",
            "\u001b[A\n",
            "  7%|6         | 11927552/170498071 [00:02<00:13, 11744839.12it/s]\n",
            "\u001b[A\n",
            "  8%|7         | 13205504/170498071 [00:02<00:14, 10873050.56it/s]\n",
            "\u001b[A\n",
            "  9%|8         | 14843904/170498071 [00:02<00:12, 12173215.71it/s]\n",
            "\u001b[A\n",
            "  9%|9         | 16154624/170498071 [00:02<00:13, 11323512.00it/s]\n",
            "\u001b[A\n",
            " 10%|#         | 17727488/170498071 [00:02<00:12, 12379731.35it/s]\n",
            "\u001b[A\n",
            " 11%|#1        | 19038208/170498071 [00:03<00:13, 11493850.77it/s]\n",
            "\u001b[A\n",
            " 12%|#2        | 20611072/170498071 [00:03<00:11, 12558874.12it/s]\n",
            "\u001b[A\n",
            " 13%|#2        | 21921792/170498071 [00:03<00:12, 11571574.09it/s]\n",
            "\u001b[A\n",
            " 14%|#3        | 23527424/170498071 [00:03<00:11, 12619956.67it/s]\n",
            "\u001b[A\n",
            " 15%|#4        | 24838144/170498071 [00:03<00:12, 11778455.27it/s]\n",
            "\u001b[A\n",
            " 16%|#5        | 26443776/170498071 [00:03<00:11, 12868785.38it/s]\n",
            "\u001b[A\n",
            " 16%|#6        | 27787264/170498071 [00:03<00:11, 11948254.46it/s]\n",
            "\u001b[A\n",
            " 17%|#7        | 29360128/170498071 [00:03<00:10, 12931644.53it/s]\n",
            "\u001b[A\n",
            " 18%|#8        | 30703616/170498071 [00:03<00:11, 12194281.18it/s]\n",
            "\u001b[A\n",
            " 19%|#8        | 32145408/170498071 [00:04<00:10, 12771082.68it/s]\n",
            "\u001b[A\n",
            " 20%|#9        | 33456128/170498071 [00:04<00:11, 11980589.35it/s]\n",
            "\u001b[A\n",
            " 20%|##        | 34897920/170498071 [00:04<00:10, 12633293.68it/s]\n",
            "\u001b[A\n",
            " 21%|##1       | 36208640/170498071 [00:04<00:11, 11828573.07it/s]\n",
            "\u001b[A\n",
            " 22%|##2       | 37814272/170498071 [00:04<00:10, 12735592.09it/s]\n",
            "\u001b[A\n",
            " 23%|##3       | 39256064/170498071 [00:04<00:10, 12370037.67it/s]\n",
            "\u001b[A\n",
            " 24%|##3       | 40665088/170498071 [00:04<00:10, 12693486.37it/s]\n",
            "\u001b[A\n",
            " 25%|##4       | 42139648/170498071 [00:04<00:10, 12339045.99it/s]\n",
            "\u001b[A\n",
            " 26%|##5       | 43745280/170498071 [00:04<00:09, 13196989.92it/s]\n",
            "\u001b[A\n",
            " 27%|##6       | 45219840/170498071 [00:05<00:09, 12641268.68it/s]\n",
            "\u001b[A\n",
            " 27%|##7       | 46727168/170498071 [00:05<00:09, 12917207.84it/s]\n",
            "\u001b[A\n",
            " 28%|##8       | 48201728/170498071 [00:05<00:09, 12718644.97it/s]\n",
            "\u001b[A\n",
            " 29%|##9       | 49741824/170498071 [00:05<00:09, 12995657.50it/s]\n",
            "\u001b[A\n",
            " 30%|###       | 51150848/170498071 [00:05<00:09, 12533133.82it/s]\n",
            "\u001b[A\n",
            " 31%|###       | 52592640/170498071 [00:05<00:09, 12741471.93it/s]\n",
            "\u001b[A\n",
            " 32%|###1      | 54001664/170498071 [00:05<00:09, 12353251.79it/s]\n",
            "\u001b[A\n",
            " 33%|###2      | 55607296/170498071 [00:05<00:09, 12752518.65it/s]\n",
            "\u001b[A\n",
            " 33%|###3      | 57016320/170498071 [00:06<00:09, 12541688.37it/s]\n",
            "\u001b[A\n",
            " 34%|###4      | 58425344/170498071 [00:06<00:08, 12492379.28it/s]\n",
            "\u001b[A\n",
            " 35%|###5      | 59899904/170498071 [00:06<00:08, 12393334.39it/s]\n",
            "\u001b[A\n",
            " 36%|###5      | 61374464/170498071 [00:06<00:08, 12504675.55it/s]\n",
            "\u001b[A\n",
            " 37%|###6      | 62849024/170498071 [00:06<00:08, 12402802.32it/s]\n",
            "\u001b[A\n",
            " 38%|###7      | 64290816/170498071 [00:06<00:08, 12436375.31it/s]\n",
            "\u001b[A\n",
            " 39%|###8      | 65765376/170498071 [00:06<00:08, 12452065.49it/s]\n",
            "\u001b[A\n",
            " 39%|###9      | 67305472/170498071 [00:06<00:08, 12584549.08it/s]\n",
            "\u001b[A\n",
            " 40%|####      | 68747264/170498071 [00:06<00:08, 12684437.80it/s]\n",
            "\u001b[A\n",
            " 41%|####1     | 70221824/170498071 [00:07<00:08, 12406203.48it/s]\n",
            "\u001b[A\n",
            " 42%|####2     | 71663616/170498071 [00:07<00:07, 12698126.98it/s]\n",
            "\u001b[A\n",
            " 43%|####2     | 73072640/170498071 [00:07<00:07, 12247192.78it/s]\n",
            "\u001b[A\n",
            " 44%|####3     | 74514432/170498071 [00:07<00:07, 12610707.36it/s]\n",
            "\u001b[A\n",
            " 45%|####4     | 76054528/170498071 [00:07<00:07, 12468322.09it/s]\n",
            "\u001b[A\n",
            " 45%|####5     | 77561856/170498071 [00:07<00:07, 12983676.94it/s]\n",
            "\u001b[A\n",
            " 46%|####6     | 79134720/170498071 [00:07<00:07, 12835263.34it/s]\n",
            "\u001b[A\n",
            " 47%|####7     | 80642048/170498071 [00:07<00:06, 13257824.86it/s]\n",
            "\u001b[A\n",
            " 48%|####8     | 82182144/170498071 [00:08<00:06, 12986387.94it/s]\n",
            "\u001b[A\n",
            " 49%|####9     | 83787776/170498071 [00:08<00:06, 13611474.60it/s]\n",
            "\u001b[A\n",
            " 50%|#####     | 85360640/170498071 [00:08<00:06, 13179084.04it/s]\n",
            "\u001b[A\n",
            " 51%|#####1    | 86999040/170498071 [00:08<00:05, 13921601.47it/s]\n",
            "\u001b[A\n",
            " 52%|#####1    | 88539136/170498071 [00:08<00:06, 13271433.11it/s]\n",
            "\u001b[A\n",
            " 53%|#####2    | 90046464/170498071 [00:08<00:05, 13732427.04it/s]\n",
            "\u001b[A\n",
            " 54%|#####3    | 91750400/170498071 [00:08<00:05, 13533680.42it/s]\n",
            "\u001b[A\n",
            " 55%|#####4    | 93421568/170498071 [00:08<00:05, 14229128.29it/s]\n",
            "\u001b[A\n",
            " 56%|#####5    | 94896128/170498071 [00:08<00:05, 13383415.52it/s]\n",
            "\u001b[A\n",
            " 57%|#####6    | 96436224/170498071 [00:09<00:05, 13819691.36it/s]\n",
            "\u001b[A\n",
            " 58%|#####7    | 98107392/170498071 [00:09<00:05, 13622884.00it/s]\n",
            "\u001b[A\n",
            " 59%|#####8    | 99745792/170498071 [00:09<00:05, 14095094.95it/s]\n",
            "\u001b[A\n",
            " 59%|#####9    | 101416960/170498071 [00:09<00:04, 13855066.02it/s]\n",
            "\u001b[A\n",
            " 60%|######    | 102989824/170498071 [00:09<00:04, 14207198.54it/s]\n",
            "\u001b[A\n",
            " 61%|######1   | 104595456/170498071 [00:09<00:04, 13732303.93it/s]\n",
            "\u001b[A\n",
            " 62%|######2   | 106201088/170498071 [00:09<00:04, 14138363.54it/s]\n",
            "\u001b[A\n",
            " 63%|######3   | 108068864/170498071 [00:09<00:04, 14227637.24it/s]\n",
            "\u001b[A\n",
            " 64%|######4   | 109608960/170498071 [00:10<00:04, 14481160.80it/s]\n",
            "\u001b[A\n",
            " 65%|######5   | 111083520/170498071 [00:10<00:04, 13521823.75it/s]\n",
            "\u001b[A\n",
            " 66%|######6   | 112885760/170498071 [00:10<00:03, 14567088.26it/s]\n",
            "\u001b[A\n",
            " 67%|######7   | 114425856/170498071 [00:10<00:04, 13737009.90it/s]\n",
            "\u001b[A\n",
            " 68%|######7   | 115900416/170498071 [00:10<00:03, 13957351.97it/s]\n",
            "\u001b[A\n",
            " 69%|######8   | 117571584/170498071 [00:10<00:03, 13526747.30it/s]\n",
            "\u001b[A\n",
            " 70%|######9   | 119144448/170498071 [00:10<00:03, 14101953.27it/s]\n",
            "\u001b[A\n",
            " 71%|#######   | 121044992/170498071 [00:10<00:03, 13940653.20it/s]\n",
            "\u001b[A\n",
            " 72%|#######2  | 123076608/170498071 [00:10<00:03, 15589549.43it/s]\n",
            "\u001b[A\n",
            " 73%|#######3  | 124682240/170498071 [00:11<00:03, 14408187.86it/s]\n",
            "\u001b[A\n",
            " 74%|#######4  | 126484480/170498071 [00:11<00:02, 15347555.08it/s]\n",
            "\u001b[A\n",
            " 75%|#######5  | 128057344/170498071 [00:11<00:03, 13866341.57it/s]\n",
            "\u001b[A\n",
            " 76%|#######5  | 129499136/170498071 [00:11<00:02, 13992963.51it/s]\n",
            "\u001b[A\n",
            " 77%|#######6  | 130940928/170498071 [00:11<00:03, 13087798.26it/s]\n",
            "\u001b[A\n",
            " 78%|#######7  | 132382720/170498071 [00:11<00:02, 13415793.66it/s]\n",
            "\u001b[A\n",
            " 78%|#######8  | 133758976/170498071 [00:11<00:02, 12513351.30it/s]\n",
            "\u001b[A\n",
            " 79%|#######9  | 135266304/170498071 [00:11<00:02, 13185309.37it/s]\n",
            "\u001b[A\n",
            " 80%|########  | 136642560/170498071 [00:12<00:02, 12207697.05it/s]\n",
            "\u001b[A\n",
            " 81%|########1 | 138182656/170498071 [00:12<00:02, 13020575.89it/s]\n",
            "\u001b[A\n",
            " 82%|########1 | 139526144/170498071 [00:12<00:02, 12102848.67it/s]\n",
            "\u001b[A\n",
            " 83%|########2 | 141066240/170498071 [00:12<00:02, 12964579.12it/s]\n",
            "\u001b[A\n",
            " 84%|########3 | 142409728/170498071 [00:12<00:02, 12028869.06it/s]\n",
            "\u001b[A\n",
            " 84%|########4 | 144048128/170498071 [00:12<00:02, 13175010.78it/s]\n",
            "\u001b[A\n",
            " 85%|########5 | 145424384/170498071 [00:12<00:02, 12029500.59it/s]\n",
            "\u001b[A\n",
            " 86%|########6 | 147095552/170498071 [00:12<00:01, 13245853.73it/s]\n",
            "\u001b[A\n",
            " 87%|########7 | 148471808/170498071 [00:12<00:01, 12081026.20it/s]\n",
            "\u001b[A\n",
            " 88%|########8 | 150175744/170498071 [00:13<00:01, 13366539.63it/s]\n",
            "\u001b[A\n",
            " 89%|########8 | 151584768/170498071 [00:13<00:01, 12208333.90it/s]\n",
            "\u001b[A\n",
            " 90%|########9 | 153124864/170498071 [00:13<00:01, 12977854.16it/s]\n",
            "\u001b[A\n",
            " 91%|######### | 154501120/170498071 [00:13<00:01, 12234676.58it/s]\n",
            "\u001b[A\n",
            " 91%|#########1| 155975680/170498071 [00:13<00:01, 12827176.60it/s]\n",
            "\u001b[A\n",
            " 92%|#########2| 157319168/170498071 [00:13<00:01, 12185847.03it/s]\n",
            "\u001b[A\n",
            " 93%|#########3| 158629888/170498071 [00:13<00:00, 12326655.06it/s]\n",
            "\u001b[A\n",
            " 94%|#########3| 159907840/170498071 [00:13<00:00, 11859400.73it/s]\n",
            "\u001b[A\n",
            " 95%|#########4| 161251328/170498071 [00:13<00:00, 12208726.33it/s]\n",
            "\u001b[A\n",
            " 95%|#########5| 162496512/170498071 [00:14<00:00, 11594726.39it/s]\n",
            "\u001b[A\n",
            " 96%|#########6| 163872768/170498071 [00:14<00:00, 12184508.96it/s]\n",
            "\u001b[A\n",
            " 97%|#########6| 165117952/170498071 [00:14<00:00, 11457722.66it/s]\n",
            "\u001b[A\n",
            " 98%|#########7| 166625280/170498071 [00:14<00:00, 12402128.88it/s]\n",
            "\u001b[A\n",
            " 98%|#########8| 167903232/170498071 [00:14<00:00, 11456796.39it/s]\n",
            "\u001b[A\n",
            " 99%|#########9| 169508864/170498071 [00:14<00:00, 12668313.12it/s]\n",
            "\u001b[A\n",
            "100%|##########| 170498071/170498071 [00:14<00:00, 11546648.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.011479155095565408\n",
            "Epoch: 1 | train_loss: 2.1663 | train_acc: 0.1522 | test_loss: 3.7266 | test_acc: 0.1201 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479159672797501\n",
            "Epoch: 2 | train_loss: 2.1475 | train_acc: 0.1616 | test_loss: 2.1935 | test_acc: 0.1563 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479173404493392\n",
            "Epoch: 3 | train_loss: 2.1492 | train_acc: 0.1604 | test_loss: 2.1506 | test_acc: 0.1639 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479196290652138\n",
            "Epoch: 4 | train_loss: 2.1537 | train_acc: 0.1602 | test_loss: 2.1610 | test_acc: 0.1673 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479228331272351\n",
            "Epoch: 5 | train_loss: 2.1435 | train_acc: 0.1641 | test_loss: 2.3764 | test_acc: 0.1124 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479269526351754\n",
            "Epoch: 6 | train_loss: 2.1530 | train_acc: 0.1605 | test_loss: 2.1352 | test_acc: 0.1650 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479319875887684\n",
            "Epoch: 7 | train_loss: 2.2893 | train_acc: 0.1074 | test_loss: 2.3055 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479379379876864\n",
            "Epoch: 8 | train_loss: 2.3057 | train_acc: 0.1003 | test_loss: 2.3042 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479448038315243\n",
            "Epoch: 9 | train_loss: 2.3053 | train_acc: 0.1011 | test_loss: 2.3065 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479525851198213\n",
            "Epoch: 10 | train_loss: 2.3056 | train_acc: 0.0985 | test_loss: 2.3060 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479612818520724\n",
            "Epoch: 11 | train_loss: 2.3056 | train_acc: 0.0997 | test_loss: 2.3041 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.01147970894027689\n",
            "Epoch: 12 | train_loss: 2.3057 | train_acc: 0.0988 | test_loss: 2.3049 | test_acc: 0.1000 |lr: 0.0115\n",
            "Learning Rate:\n",
            "0.011479814216460438\n",
            "Epoch: 13 | train_loss: 2.3058 | train_acc: 0.1011 | test_loss: 2.3061 | test_acc: 0.1000 |lr: 0.0115\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.002023036035985762\n",
            "Epoch: 1 | train_loss: 1.6595 | train_acc: 0.3786 | test_loss: 1.7199 | test_acc: 0.3738 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020230489243174624\n",
            "Epoch: 2 | train_loss: 1.2704 | train_acc: 0.5697 | test_loss: 1.5688 | test_acc: 0.4834 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020230875892988665\n",
            "Epoch: 3 | train_loss: 1.2212 | train_acc: 0.5924 | test_loss: 1.5080 | test_acc: 0.4579 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020231520308889236\n",
            "Epoch: 4 | train_loss: 1.1910 | train_acc: 0.6065 | test_loss: 1.4756 | test_acc: 0.4903 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020232422490192234\n",
            "Epoch: 5 | train_loss: 1.1662 | train_acc: 0.6158 | test_loss: 1.6337 | test_acc: 0.4407 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020233582435939604\n",
            "Epoch: 6 | train_loss: 1.1405 | train_acc: 0.6242 | test_loss: 1.4087 | test_acc: 0.5317 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020235000144899623\n",
            "Epoch: 7 | train_loss: 1.1238 | train_acc: 0.6319 | test_loss: 1.4772 | test_acc: 0.5148 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020236675615567176\n",
            "Epoch: 8 | train_loss: 1.1174 | train_acc: 0.6344 | test_loss: 1.5735 | test_acc: 0.4571 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.002023860884616306\n",
            "Epoch: 9 | train_loss: 1.1252 | train_acc: 0.6293 | test_loss: 1.2439 | test_acc: 0.5831 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020240799834634754\n",
            "Epoch: 10 | train_loss: 1.1290 | train_acc: 0.6303 | test_loss: 1.8158 | test_acc: 0.4498 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.002024324857865571\n",
            "Epoch: 11 | train_loss: 1.1106 | train_acc: 0.6353 | test_loss: 1.6913 | test_acc: 0.4443 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.0020245955075626\n",
            "Epoch: 12 | train_loss: 1.1082 | train_acc: 0.6387 | test_loss: 1.4486 | test_acc: 0.5233 |lr: 0.0020\n",
            "Learning Rate:\n",
            "0.002024891932267174\n",
            "Epoch: 13 | train_loss: 1.1137 | train_acc: 0.6369 | test_loss: 1.4343 | test_acc: 0.5322 |lr: 0.0020\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.0018436071987538924\n",
            "Epoch: 1 | train_loss: 1.7394 | train_acc: 0.3465 | test_loss: 1.7244 | test_acc: 0.3919 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.001843620983838265\n",
            "Epoch: 2 | train_loss: 1.4144 | train_acc: 0.5236 | test_loss: 1.5177 | test_acc: 0.5305 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018436623390741952\n",
            "Epoch: 3 | train_loss: 1.3836 | train_acc: 0.5515 | test_loss: 1.6142 | test_acc: 0.4582 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018437312644101547\n",
            "Epoch: 4 | train_loss: 1.3440 | train_acc: 0.5707 | test_loss: 1.5783 | test_acc: 0.4450 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.00184382775976024\n",
            "Epoch: 5 | train_loss: 1.3072 | train_acc: 0.5850 | test_loss: 1.5106 | test_acc: 0.5056 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018439518250042072\n",
            "Epoch: 6 | train_loss: 1.2997 | train_acc: 0.5881 | test_loss: 1.4478 | test_acc: 0.5033 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018441034599874367\n",
            "Epoch: 7 | train_loss: 1.2896 | train_acc: 0.5962 | test_loss: 1.6821 | test_acc: 0.4336 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018442826645209617\n",
            "Epoch: 8 | train_loss: 1.2953 | train_acc: 0.5968 | test_loss: 1.5300 | test_acc: 0.5011 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018444894383814608\n",
            "Epoch: 9 | train_loss: 1.2984 | train_acc: 0.5992 | test_loss: 1.5892 | test_acc: 0.4557 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018447237813112444\n",
            "Epoch: 10 | train_loss: 1.2865 | train_acc: 0.6089 | test_loss: 1.8844 | test_acc: 0.3829 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.001844985693018282\n",
            "Epoch: 11 | train_loss: 1.2931 | train_acc: 0.6040 | test_loss: 1.4080 | test_acc: 0.5765 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018452751731761752\n",
            "Epoch: 12 | train_loss: 1.2814 | train_acc: 0.6120 | test_loss: 1.5822 | test_acc: 0.4943 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018455922214241638\n",
            "Epoch: 13 | train_loss: 1.2924 | train_acc: 0.6069 | test_loss: 1.4739 | test_acc: 0.5085 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018459368373671473\n",
            "Epoch: 14 | train_loss: 1.2883 | train_acc: 0.6105 | test_loss: 1.7354 | test_acc: 0.3938 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018463090205756705\n",
            "Epoch: 15 | train_loss: 1.2993 | train_acc: 0.6080 | test_loss: 1.6855 | test_acc: 0.4521 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.001846708770585903\n",
            "Epoch: 16 | train_loss: 1.3034 | train_acc: 0.6047 | test_loss: 2.2150 | test_acc: 0.3094 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018471360868996742\n",
            "Epoch: 17 | train_loss: 1.2985 | train_acc: 0.6073 | test_loss: 1.4754 | test_acc: 0.5271 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018475909689844791\n",
            "Epoch: 18 | train_loss: 1.3106 | train_acc: 0.5995 | test_loss: 1.8446 | test_acc: 0.3978 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018480734162734172\n",
            "Epoch: 19 | train_loss: 1.3036 | train_acc: 0.6019 | test_loss: 1.7790 | test_acc: 0.4389 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.001848583428165268\n",
            "Epoch: 20 | train_loss: 1.3070 | train_acc: 0.6027 | test_loss: 1.7107 | test_acc: 0.4610 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018491210040244566\n",
            "Epoch: 21 | train_loss: 1.3202 | train_acc: 0.5963 | test_loss: 1.8703 | test_acc: 0.4097 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.0018496861431810535\n",
            "Epoch: 22 | train_loss: 1.3091 | train_acc: 0.6021 | test_loss: 1.5709 | test_acc: 0.4927 |lr: 0.0018\n",
            "Learning Rate:\n",
            "0.001850278844930768\n",
            "Epoch: 23 | train_loss: 1.3198 | train_acc: 0.5962 | test_loss: 2.0496 | test_acc: 0.3584 |lr: 0.0019\n",
            "Learning Rate:\n",
            "0.0018508991085349896\n",
            "Epoch: 24 | train_loss: 1.3144 | train_acc: 0.6036 | test_loss: 1.4992 | test_acc: 0.5136 |lr: 0.0019\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.006038477096802164\n",
            "Epoch: 1 | train_loss: 1.6723 | train_acc: 0.3772 | test_loss: 1.5295 | test_acc: 0.4920 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.0060385187068850155\n",
            "Epoch: 2 | train_loss: 1.1185 | train_acc: 0.6135 | test_loss: 1.1837 | test_acc: 0.6125 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006038643537085803\n",
            "Epoch: 3 | train_loss: 0.9083 | train_acc: 0.6921 | test_loss: 0.9533 | test_acc: 0.6831 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.00603885158726114\n",
            "Epoch: 4 | train_loss: 0.7923 | train_acc: 0.7372 | test_loss: 0.8246 | test_acc: 0.7221 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006039142857172108\n",
            "Epoch: 5 | train_loss: 0.7242 | train_acc: 0.7620 | test_loss: 0.7497 | test_acc: 0.7476 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006039517346484169\n",
            "Epoch: 6 | train_loss: 0.6688 | train_acc: 0.7793 | test_loss: 0.7461 | test_acc: 0.7519 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006039975054767249\n",
            "Epoch: 7 | train_loss: 0.6311 | train_acc: 0.7920 | test_loss: 0.6816 | test_acc: 0.7721 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006040515981495659\n",
            "Epoch: 8 | train_loss: 0.6026 | train_acc: 0.8013 | test_loss: 0.6411 | test_acc: 0.7881 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006041140126048228\n",
            "Epoch: 9 | train_loss: 0.5688 | train_acc: 0.8137 | test_loss: 0.8898 | test_acc: 0.7104 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.0060418474877080575\n",
            "Epoch: 10 | train_loss: 0.5558 | train_acc: 0.8193 | test_loss: 0.7943 | test_acc: 0.7447 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006042638065662825\n",
            "Epoch: 11 | train_loss: 0.5317 | train_acc: 0.8268 | test_loss: 0.6368 | test_acc: 0.7882 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006043511859004563\n",
            "Epoch: 12 | train_loss: 0.5256 | train_acc: 0.8275 | test_loss: 0.9706 | test_acc: 0.7104 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006044468866729713\n",
            "Epoch: 13 | train_loss: 0.5170 | train_acc: 0.8330 | test_loss: 0.7173 | test_acc: 0.7679 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.0060455090877391815\n",
            "Epoch: 14 | train_loss: 0.5070 | train_acc: 0.8338 | test_loss: 0.6446 | test_acc: 0.7849 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006046632520838396\n",
            "Epoch: 15 | train_loss: 0.4917 | train_acc: 0.8394 | test_loss: 0.6458 | test_acc: 0.7944 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006047839164737001\n",
            "Epoch: 16 | train_loss: 0.4882 | train_acc: 0.8420 | test_loss: 0.6598 | test_acc: 0.7886 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006049129018049271\n",
            "Epoch: 17 | train_loss: 0.4868 | train_acc: 0.8418 | test_loss: 0.6056 | test_acc: 0.8021 |lr: 0.0060\n",
            "Learning Rate:\n",
            "0.006050502079293835\n",
            "Epoch: 18 | train_loss: 0.4697 | train_acc: 0.8475 | test_loss: 0.7868 | test_acc: 0.7510 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.00605195834689376\n",
            "Epoch: 19 | train_loss: 0.4667 | train_acc: 0.8479 | test_loss: 0.5661 | test_acc: 0.8157 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.0060534978191766065\n",
            "Epoch: 20 | train_loss: 0.4594 | train_acc: 0.8502 | test_loss: 0.6669 | test_acc: 0.7843 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.006055120494374261\n",
            "Epoch: 21 | train_loss: 0.4700 | train_acc: 0.8476 | test_loss: 0.6066 | test_acc: 0.7991 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.006056826370623214\n",
            "Epoch: 22 | train_loss: 0.4533 | train_acc: 0.8534 | test_loss: 0.6687 | test_acc: 0.7836 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.006058615445964255\n",
            "Epoch: 23 | train_loss: 0.4533 | train_acc: 0.8535 | test_loss: 0.6123 | test_acc: 0.7988 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.006060487718342694\n",
            "Epoch: 24 | train_loss: 0.4575 | train_acc: 0.8535 | test_loss: 0.7209 | test_acc: 0.7658 |lr: 0.0061\n",
            "Learning Rate:\n",
            "0.006062443185608307\n",
            "Epoch: 25 | train_loss: 0.4559 | train_acc: 0.8525 | test_loss: 0.6221 | test_acc: 0.7948 |lr: 0.0061\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.0026478814478837903\n",
            "Epoch: 1 | train_loss: 1.7043 | train_acc: 0.3592 | test_loss: 1.2580 | test_acc: 0.5440 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.0026478885825606813\n",
            "Epoch: 2 | train_loss: 1.1865 | train_acc: 0.5912 | test_loss: 1.0261 | test_acc: 0.6510 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.002647909986588176\n",
            "Epoch: 3 | train_loss: 0.9916 | train_acc: 0.6663 | test_loss: 0.9819 | test_acc: 0.6762 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.0026479456599566298\n",
            "Epoch: 4 | train_loss: 0.8656 | train_acc: 0.7146 | test_loss: 0.8683 | test_acc: 0.7081 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.002647995602650055\n",
            "Epoch: 5 | train_loss: 0.7997 | train_acc: 0.7391 | test_loss: 0.7836 | test_acc: 0.7431 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.0026480598146459977\n",
            "Epoch: 6 | train_loss: 0.7513 | train_acc: 0.7559 | test_loss: 0.7970 | test_acc: 0.7346 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.0026481382959156335\n",
            "Epoch: 7 | train_loss: 0.7172 | train_acc: 0.7688 | test_loss: 0.7116 | test_acc: 0.7641 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.002648231046423727\n",
            "Epoch: 8 | train_loss: 0.6954 | train_acc: 0.7754 | test_loss: 0.7103 | test_acc: 0.7680 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.0026483380661286027\n",
            "Epoch: 9 | train_loss: 0.6853 | train_acc: 0.7799 | test_loss: 0.8410 | test_acc: 0.7321 |lr: 0.0026\n",
            "Learning Rate:\n",
            "0.002648459354982216\n",
            "Epoch: 10 | train_loss: 0.6643 | train_acc: 0.7844 | test_loss: 0.8039 | test_acc: 0.7357 |lr: 0.0026\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.0007609801026586607\n",
            "Epoch: 1 | train_loss: 1.6336 | train_acc: 0.3919 | test_loss: 1.3472 | test_acc: 0.5399 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007609826309607666\n",
            "Epoch: 2 | train_loss: 1.0910 | train_acc: 0.6178 | test_loss: 0.9747 | test_acc: 0.6646 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007609902158656756\n",
            "Epoch: 3 | train_loss: 0.8832 | train_acc: 0.6968 | test_loss: 0.8646 | test_acc: 0.7094 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007610028573691932\n",
            "Epoch: 4 | train_loss: 0.7682 | train_acc: 0.7409 | test_loss: 0.7454 | test_acc: 0.7506 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007610205554643215\n",
            "Epoch: 5 | train_loss: 0.6894 | train_acc: 0.7696 | test_loss: 0.7340 | test_acc: 0.7534 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007610433101412559\n",
            "Epoch: 6 | train_loss: 0.6289 | train_acc: 0.7906 | test_loss: 0.6292 | test_acc: 0.7919 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007610711213873952\n",
            "Epoch: 7 | train_loss: 0.5846 | train_acc: 0.8053 | test_loss: 0.7220 | test_acc: 0.7657 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007611039891873422\n",
            "Epoch: 8 | train_loss: 0.5540 | train_acc: 0.8159 | test_loss: 0.6776 | test_acc: 0.7786 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007611419135228996\n",
            "Epoch: 9 | train_loss: 0.5146 | train_acc: 0.8286 | test_loss: 0.6161 | test_acc: 0.7931 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007611848943730598\n",
            "Epoch: 10 | train_loss: 0.4939 | train_acc: 0.8344 | test_loss: 0.6143 | test_acc: 0.7969 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007612329317140294\n",
            "Epoch: 11 | train_loss: 0.4710 | train_acc: 0.8432 | test_loss: 0.5783 | test_acc: 0.8102 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007612860255192047\n",
            "Epoch: 12 | train_loss: 0.4553 | train_acc: 0.8504 | test_loss: 0.5386 | test_acc: 0.8234 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007613441757591856\n",
            "Epoch: 13 | train_loss: 0.4361 | train_acc: 0.8553 | test_loss: 0.5005 | test_acc: 0.8330 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007614073824017721\n",
            "Epoch: 14 | train_loss: 0.4215 | train_acc: 0.8618 | test_loss: 0.5285 | test_acc: 0.8290 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007614756454119645\n",
            "Epoch: 15 | train_loss: 0.4203 | train_acc: 0.8602 | test_loss: 0.5651 | test_acc: 0.8179 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007615489647519701\n",
            "Epoch: 16 | train_loss: 0.4019 | train_acc: 0.8673 | test_loss: 0.4978 | test_acc: 0.8353 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007616273403811755\n",
            "Epoch: 17 | train_loss: 0.3865 | train_acc: 0.8731 | test_loss: 0.5749 | test_acc: 0.8156 |lr: 0.0008\n",
            "Learning Rate:\n",
            "0.0007617107722561918\n",
            "Epoch: 18 | train_loss: 0.3811 | train_acc: 0.8745 | test_loss: 0.5351 | test_acc: 0.8206 |lr: 0.0008\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.009276223548858736\n",
            "Epoch: 1 | train_loss: 1.7303 | train_acc: 0.3404 | test_loss: 1.4465 | test_acc: 0.4778 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009276306098922271\n",
            "Epoch: 2 | train_loss: 1.3200 | train_acc: 0.5400 | test_loss: 1.4227 | test_acc: 0.4984 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009276553748990363\n",
            "Epoch: 3 | train_loss: 1.1672 | train_acc: 0.6045 | test_loss: 1.5547 | test_acc: 0.4419 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009276966498695777\n",
            "Epoch: 4 | train_loss: 1.1144 | train_acc: 0.6226 | test_loss: 1.4189 | test_acc: 0.5250 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009277544347426309\n",
            "Epoch: 5 | train_loss: 1.0833 | train_acc: 0.6360 | test_loss: 1.8654 | test_acc: 0.4381 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009278287294324866\n",
            "Epoch: 6 | train_loss: 1.0598 | train_acc: 0.6446 | test_loss: 1.8940 | test_acc: 0.4225 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009279195338289581\n",
            "Epoch: 7 | train_loss: 1.0502 | train_acc: 0.6484 | test_loss: 1.4803 | test_acc: 0.4959 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009280268477973586\n",
            "Epoch: 8 | train_loss: 1.0364 | train_acc: 0.6574 | test_loss: 1.6571 | test_acc: 0.4191 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009281506711785265\n",
            "Epoch: 9 | train_loss: 1.0319 | train_acc: 0.6546 | test_loss: 1.6678 | test_acc: 0.4543 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009282910037888115\n",
            "Epoch: 10 | train_loss: 1.0281 | train_acc: 0.6625 | test_loss: 1.3651 | test_acc: 0.5646 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.00928447845420069\n",
            "Epoch: 11 | train_loss: 1.0049 | train_acc: 0.6700 | test_loss: 1.5257 | test_acc: 0.4940 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.00928621195839674\n",
            "Epoch: 12 | train_loss: 1.0081 | train_acc: 0.6678 | test_loss: 1.3277 | test_acc: 0.5642 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.00928811054790521\n",
            "Epoch: 13 | train_loss: 1.0087 | train_acc: 0.6725 | test_loss: 1.5171 | test_acc: 0.5027 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009290174219910075\n",
            "Epoch: 14 | train_loss: 1.0089 | train_acc: 0.6712 | test_loss: 1.4183 | test_acc: 0.5191 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009292402971350588\n",
            "Epoch: 15 | train_loss: 1.0104 | train_acc: 0.6720 | test_loss: 1.6416 | test_acc: 0.5054 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009294796798921118\n",
            "Epoch: 16 | train_loss: 1.0097 | train_acc: 0.6727 | test_loss: 1.2935 | test_acc: 0.5711 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009297355699071086\n",
            "Epoch: 17 | train_loss: 0.9969 | train_acc: 0.6776 | test_loss: 1.1919 | test_acc: 0.6181 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009300079668005279\n",
            "Epoch: 18 | train_loss: 1.0033 | train_acc: 0.6760 | test_loss: 1.4743 | test_acc: 0.5213 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009302968701683512\n",
            "Epoch: 19 | train_loss: 1.0055 | train_acc: 0.6751 | test_loss: 1.6687 | test_acc: 0.4557 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009306022795820768\n",
            "Epoch: 20 | train_loss: 1.0114 | train_acc: 0.6716 | test_loss: 1.2699 | test_acc: 0.5798 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009309241945887309\n",
            "Epoch: 21 | train_loss: 0.9962 | train_acc: 0.6778 | test_loss: 1.7239 | test_acc: 0.5149 |lr: 0.0093\n",
            "Learning Rate:\n",
            "0.009312626147108566\n",
            "Epoch: 22 | train_loss: 1.0117 | train_acc: 0.6719 | test_loss: 2.3969 | test_acc: 0.4056 |lr: 0.0093\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.013055363400152786\n",
            "Epoch: 1 | train_loss: 2.1934 | train_acc: 0.1569 | test_loss: 2.1604 | test_acc: 0.1627 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305536758179987\n",
            "Epoch: 2 | train_loss: 2.1729 | train_acc: 0.1637 | test_loss: 2.2204 | test_acc: 0.1447 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055380126740845\n",
            "Epoch: 3 | train_loss: 2.2085 | train_acc: 0.1495 | test_loss: 2.3038 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.0130554010349751\n",
            "Epoch: 4 | train_loss: 2.3039 | train_acc: 0.0987 | test_loss: 2.3038 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305543030650147\n",
            "Epoch: 5 | train_loss: 2.3039 | train_acc: 0.1018 | test_loss: 2.3033 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055467941318344\n",
            "Epoch: 6 | train_loss: 2.3040 | train_acc: 0.0991 | test_loss: 2.3037 | test_acc: 0.0998 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305551393942389\n",
            "Epoch: 7 | train_loss: 2.3041 | train_acc: 0.0996 | test_loss: 2.3039 | test_acc: 0.1001 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055568300815501\n",
            "Epoch: 8 | train_loss: 2.3041 | train_acc: 0.0977 | test_loss: 2.3038 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055631025490289\n",
            "Epoch: 9 | train_loss: 2.3040 | train_acc: 0.1005 | test_loss: 2.3037 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055702113444978\n",
            "Epoch: 10 | train_loss: 2.3042 | train_acc: 0.1005 | test_loss: 2.3047 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055781564675684\n",
            "Epoch: 11 | train_loss: 2.3045 | train_acc: 0.0970 | test_loss: 2.3032 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013055869379178187\n",
            "Epoch: 12 | train_loss: 2.3041 | train_acc: 0.0999 | test_loss: 2.3036 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305596555694788\n",
            "Epoch: 13 | train_loss: 2.3043 | train_acc: 0.0986 | test_loss: 2.3037 | test_acc: 0.1002 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056070097979544\n",
            "Epoch: 14 | train_loss: 2.3042 | train_acc: 0.0989 | test_loss: 2.3049 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305618300226763\n",
            "Epoch: 15 | train_loss: 2.3043 | train_acc: 0.0983 | test_loss: 2.3052 | test_acc: 0.0998 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056304269806085\n",
            "Epoch: 16 | train_loss: 2.3041 | train_acc: 0.1007 | test_loss: 2.3030 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056433900588416\n",
            "Epoch: 17 | train_loss: 2.3042 | train_acc: 0.0995 | test_loss: 2.3057 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056571894607794\n",
            "Epoch: 18 | train_loss: 2.3043 | train_acc: 0.0971 | test_loss: 2.3040 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056718251856836\n",
            "Epoch: 19 | train_loss: 2.3042 | train_acc: 0.0983 | test_loss: 2.3036 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013056872972327604\n",
            "Epoch: 20 | train_loss: 2.3041 | train_acc: 0.0968 | test_loss: 2.3049 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305703605601194\n",
            "Epoch: 21 | train_loss: 2.3042 | train_acc: 0.1001 | test_loss: 2.3041 | test_acc: 0.1002 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.01305720750290118\n",
            "Epoch: 22 | train_loss: 2.3038 | train_acc: 0.1002 | test_loss: 2.3040 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013057387312986113\n",
            "Epoch: 23 | train_loss: 2.3040 | train_acc: 0.1011 | test_loss: 2.3039 | test_acc: 0.0999 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013057575486257134\n",
            "Epoch: 24 | train_loss: 2.3039 | train_acc: 0.0972 | test_loss: 2.3062 | test_acc: 0.1001 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013057772022704195\n",
            "Epoch: 25 | train_loss: 2.3042 | train_acc: 0.0997 | test_loss: 2.3037 | test_acc: 0.1004 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013057976922316805\n",
            "Epoch: 26 | train_loss: 2.3041 | train_acc: 0.0990 | test_loss: 2.3046 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013058190185084029\n",
            "Epoch: 27 | train_loss: 2.3043 | train_acc: 0.0985 | test_loss: 2.3029 | test_acc: 0.1002 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013058411810994486\n",
            "Epoch: 28 | train_loss: 2.3040 | train_acc: 0.0986 | test_loss: 2.3050 | test_acc: 0.1000 |lr: 0.0131\n",
            "Learning Rate:\n",
            "0.013058641800036408\n",
            "Epoch: 29 | train_loss: 2.3039 | train_acc: 0.1005 | test_loss: 2.3047 | test_acc: 0.1001 |lr: 0.0131\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.0034429481949133506\n",
            "Epoch: 1 | train_loss: 1.6292 | train_acc: 0.3934 | test_loss: 1.5465 | test_acc: 0.4725 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034429671067567474\n",
            "Epoch: 2 | train_loss: 1.1148 | train_acc: 0.6144 | test_loss: 1.0749 | test_acc: 0.6297 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034430238422696186\n",
            "Epoch: 3 | train_loss: 0.9347 | train_acc: 0.6864 | test_loss: 0.8883 | test_acc: 0.6967 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.003443118401400047\n",
            "Epoch: 4 | train_loss: 0.8361 | train_acc: 0.7243 | test_loss: 0.8795 | test_acc: 0.7085 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034432507840614496\n",
            "Epoch: 5 | train_loss: 0.7720 | train_acc: 0.7469 | test_loss: 0.8185 | test_acc: 0.7357 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034434209901326174\n",
            "Epoch: 6 | train_loss: 0.7220 | train_acc: 0.7643 | test_loss: 0.7389 | test_acc: 0.7589 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034436290194577585\n",
            "Epoch: 7 | train_loss: 0.7047 | train_acc: 0.7711 | test_loss: 0.9761 | test_acc: 0.6819 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034438748718464\n",
            "Epoch: 8 | train_loss: 0.6890 | train_acc: 0.7767 | test_loss: 0.9979 | test_acc: 0.6801 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034441585470734726\n",
            "Epoch: 9 | train_loss: 0.6731 | train_acc: 0.7812 | test_loss: 0.7950 | test_acc: 0.7468 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034444800448792945\n",
            "Epoch: 10 | train_loss: 0.6668 | train_acc: 0.7858 | test_loss: 0.8354 | test_acc: 0.7311 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.003444839364969518\n",
            "Epoch: 11 | train_loss: 0.6562 | train_acc: 0.7879 | test_loss: 0.7290 | test_acc: 0.7612 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.003445236507015184\n",
            "Epoch: 12 | train_loss: 0.6588 | train_acc: 0.7882 | test_loss: 0.7180 | test_acc: 0.7685 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034456714706527503\n",
            "Epoch: 13 | train_loss: 0.6415 | train_acc: 0.7949 | test_loss: 0.6759 | test_acc: 0.7758 |lr: 0.0034\n",
            "Learning Rate:\n",
            "0.0034461442554839655\n",
            "Epoch: 14 | train_loss: 0.6377 | train_acc: 0.7971 | test_loss: 0.8866 | test_acc: 0.7099 |lr: 0.0034\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.0010401521065280701\n",
            "Epoch: 1 | train_loss: 1.6586 | train_acc: 0.3869 | test_loss: 1.6022 | test_acc: 0.4254 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.001040154645334284\n",
            "Epoch: 2 | train_loss: 1.2306 | train_acc: 0.5862 | test_loss: 1.3194 | test_acc: 0.5459 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010401622617518987\n",
            "Epoch: 3 | train_loss: 1.1643 | train_acc: 0.6229 | test_loss: 1.3995 | test_acc: 0.5631 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.001040174955777809\n",
            "Epoch: 4 | train_loss: 1.1396 | train_acc: 0.6356 | test_loss: 1.5998 | test_acc: 0.4857 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010401927274068593\n",
            "Epoch: 5 | train_loss: 1.1154 | train_acc: 0.6443 | test_loss: 1.3545 | test_acc: 0.5422 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010402155766318123\n",
            "Epoch: 6 | train_loss: 1.0853 | train_acc: 0.6535 | test_loss: 1.1920 | test_acc: 0.6154 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010402435034433735\n",
            "Epoch: 7 | train_loss: 1.0716 | train_acc: 0.6615 | test_loss: 1.6339 | test_acc: 0.4830 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010402765078301873\n",
            "Epoch: 8 | train_loss: 1.0687 | train_acc: 0.6614 | test_loss: 1.2359 | test_acc: 0.5862 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.001040314589778827\n",
            "Epoch: 9 | train_loss: 1.0649 | train_acc: 0.6633 | test_loss: 1.2461 | test_acc: 0.5750 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.001040357749273798\n",
            "Epoch: 10 | train_loss: 1.0639 | train_acc: 0.6635 | test_loss: 1.2477 | test_acc: 0.5854 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010404059862975414\n",
            "Epoch: 11 | train_loss: 1.0611 | train_acc: 0.6665 | test_loss: 1.1544 | test_acc: 0.6322 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010404593008304375\n",
            "Epoch: 12 | train_loss: 1.0634 | train_acc: 0.6646 | test_loss: 1.1985 | test_acc: 0.5949 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010405176928507988\n",
            "Epoch: 13 | train_loss: 1.0532 | train_acc: 0.6681 | test_loss: 1.3072 | test_acc: 0.5525 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.00104058116233487\n",
            "Epoch: 14 | train_loss: 1.0605 | train_acc: 0.6664 | test_loss: 1.4007 | test_acc: 0.5371 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.001040649709256828\n",
            "Epoch: 15 | train_loss: 1.0626 | train_acc: 0.6665 | test_loss: 1.2727 | test_acc: 0.5672 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010407233335887958\n",
            "Epoch: 16 | train_loss: 1.0515 | train_acc: 0.6702 | test_loss: 1.3293 | test_acc: 0.5646 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010408020353008147\n",
            "Epoch: 17 | train_loss: 1.0629 | train_acc: 0.6665 | test_loss: 1.3542 | test_acc: 0.5406 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010408858143608757\n",
            "Epoch: 18 | train_loss: 1.0630 | train_acc: 0.6663 | test_loss: 1.5091 | test_acc: 0.4784 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010409746707348948\n",
            "Epoch: 19 | train_loss: 1.0647 | train_acc: 0.6661 | test_loss: 1.3677 | test_acc: 0.5575 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010410686043867239\n",
            "Epoch: 20 | train_loss: 1.0573 | train_acc: 0.6678 | test_loss: 1.1062 | test_acc: 0.6356 |lr: 0.0010\n",
            "Learning Rate:\n",
            "0.0010411676152781504\n",
            "Epoch: 21 | train_loss: 1.0577 | train_acc: 0.6658 | test_loss: 1.3133 | test_acc: 0.5692 |lr: 0.0010\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning Rate:\n",
            "0.002381938630052864\n",
            "Epoch: 1 | train_loss: 1.8516 | train_acc: 0.2836 | test_loss: 1.6029 | test_acc: 0.3774 |lr: 0.0024\n",
            "Learning Rate:\n",
            "0.00238193874119947\n",
            " 20%|        | 10/50 [3:29:08<14:24:12, 1296.31s/trial, best loss: -0.8205613057324841]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "he4i9u75Oa_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulQXyYmqamGM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Train a classification model for the CIFAR-10 dataset.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = 12\n",
        "BATCH_SIZE = 32\n",
        "MAX_LR = 0.1 # threshold for learning_rate from the learing rate scheduler\n",
        "GRAD_CLIP = 0.1 # threshold for gradient values\n",
        "WEIGHT_DECAY = 1e-4 # regularization parameter\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Setup data directory\n",
        "    data_dir = 'data'\n",
        "\n",
        "    # Setup device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Create dataloaders from data_setup.py\n",
        "    train_dataloader, test_dataloader, class_names = data_loader(\n",
        "        data_dir=data_dir,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Create VGG16 model from model_builder.py\n",
        "    model = VGG16(num_classes=len(class_names))\n",
        "\n",
        "    # Set loss and optimizer\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=MAX_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Start training from engine.py\n",
        "    results = train(model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        test_dataloader=test_dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        device=device,\n",
        "        max_lr=MAX_LR,\n",
        "        grad_clip=GRAD_CLIP\n",
        "    )\n",
        "\n",
        "    plot_loss_curves(results)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}